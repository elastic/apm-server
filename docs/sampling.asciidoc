[[sampling]]
=== Transaction sampling

Distributed tracing can generate a substantial amount of data.
More data can mean more cost and more noise to sift through.
Sampling aims to lower the amount of data ingested and the effort required to analyze that data --
all while still making it easy to find anomalous patterns in your applications, detect outages, track errors,
and lower MTTR.

// In other words, in most cases, you can still find anomalous patterns in your applications, detect outages, track errors,
// and lower MTTR, even when sampling at less than `100%`.

Elastic APM supports two types of sampling:

* <<head-based-sampling>>
* <<tail-based-sampling>>

[float]
[[head-based-sampling]]
==== Head-based sampling

In head-based sampling, the sampling decision for each trace is made when that trace is initiated.
Each trace has a defined and equal probability of being sampled.

For example, a sampling value of `.2` indicates a transaction sample rate of `20%`.
This means that only `20%` of traces will send and retain all of their associated information.
The remaining traces will drop contextual information to reduce the transfer and storage size of the trace.

Head-based sampling is quick and easy to set up.
The downside of this type of sampling is that it's entirely random -- good
data might be discarded purely due to random chance.

See <<configure-head-based-sampling>> to get started.

**Distributed tracing with head-based sampling**

In a distributed trace, the sampling decision is still made when the trace is initiated.
Each subsequent service respects the initial service's sampling decision, regardless of its configured sample rate;
the result is a sampling percentage that matches the initiating service.

In this example, `Service A` initiates four transactions and has sample rate of `.5` (`50%`).
The sample rates of `Service B` and `Service C` are ignored.

image::./images/dt-sampling-example-1.png[Distributed tracing and head based sampling example one]

In this example, `Service A` initiates four transactions and has a sample rate of `1` (`100%`).
Again, the sample rates of `Service B` and `Service C` are ignored.

image::./images/dt-sampling-example-2.png[Distributed tracing and head based sampling example two]

[float]
[[tail-based-sampling]]
==== Tail-based sampling

In tail-based sampling, the sampling decision for each trace is made after the trace has completed.
This means all traces will be analyzed against a set of rules, or policies, which will determine the rate at which they are sampled.

Tail-based sampling reduces the risk of discarding important data, because the sampling decision is only made after
all traces have been analyzed.
However, because traces are all initially observed,
storage and transfer costs may be higher than with head-based sampling.

See <<configure-tail-based-sampling>> to get started.

**Distributed tracing with tail-based sampling**

With tail-based sampling, all traces are observed and a sampling decision is only made once a trace completes.

In this example, `Service A` initiates four transactions.
If our sample rate is `.5` (`50%`) for traces with a `success` outcome,
and `1` (`100%`) for traces with a `failure` outcome,
the sampled traces would look something like this:

image::./images/dt-sampling-example-3.png[Distributed tracing and tail based sampling example one]

[float]
=== Sampled data

A sampled trace retains all data associated with it.

Non-sampled traces drop <<transaction-spans,`span`>> data.
Spans contain more granular information about what is happening within a transaction,
like external requests or database calls.
Spans also contain contextual information and labels.

Regardless of the sampling decision, all traces retain transaction and error data.
This means the following data will always accurately reflect *all* of your application's requests, regardless of the configured sampling rate:

* Transaction duration and transactions per minute
* Transaction breakdown metrics
* Errors, error occurrence, and error rate

[float]
=== Sample rates

What's the best sampling rate? Unfortunately, there isn't one.
Sampling is dependent on your data, the throughput of your application, data retainment policies, and other factors.
A sampling rate from `.1%` to `100%` would all be considered normal.
You may even decide to have a unique sample rate per service -- for example, if a certain service
experiences considerably more or less traffic than another.

// Regardless, cost conscious customers are likely to be fine with a lower sample rate.

[float]
=== APM app implications

The APM app always knows which transactions have and haven't been sampled.
This prevents the app from showing broken traces.
In addition, because transaction and error data is never sampled,
you can always expect metrics and errors to be accurately reflected in the APM app.

*Service maps*

Service maps rely on distributed traces to draw connections between services.
A minimum required version of APM agents is required for Service maps to work.
See {kibana-ref}/service-maps.html[Service maps] for more information.

[[configure-head-based-sampling]]
==== Configure head-based sampling

There are three ways to adjust the head-based sampling rate of your APM agents:

===== Dynamic configuration

The transaction sample rate can be changed dynamically (no redeployment necessary) on a per-service and per-environment
basis with {kibana-ref}/agent-configuration.html[APM Agent Configuration] in Kibana.

===== Kibana API configuration

APM Agent configuration exposes an API that can be used to programmatically change
your agents' sampling rate.
An example is provided in the {kibana-ref}/agent-config-api.html[Agent configuration API reference].

===== APM agent configuration

Each agent provides a configuration value used to set the transaction sample rate.
See the relevant agent's documentation for more details:

* Go: {apm-go-ref-v}/configuration.html#config-transaction-sample-rate[`ELASTIC_APM_TRANSACTION_SAMPLE_RATE`]
* Java: {apm-java-ref-v}/config-core.html#config-transaction-sample-rate[`transaction_sample_rate`]
* .NET: {apm-dotnet-ref-v}/config-core.html#config-transaction-sample-rate[`TransactionSampleRate`]
* Node.js: {apm-node-ref-v}/configuration.html#transaction-sample-rate[`transactionSampleRate`]
* PHP: {apm-php-ref-v}/configuration-reference.html#config-transaction-sample-rate[`transaction_sample_rate`]
* Python: {apm-py-ref-v}/configuration.html#config-transaction-sample-rate[`transaction_sample_rate`]
* Ruby: {apm-ruby-ref-v}/configuration.html#config-transaction-sample-rate[`transaction_sample_rate`]

[[configure-tail-based-sampling]]
==== Configure tail-based sampling

Tail-based sampling is enabled in the APM integration settings.

When enabled, trace events are mapped to sampling policies.
Each sampling policy must specify a sample rate, and can optionally specify other conditions.
All of the policy conditions must be true for a trace event to match it.

Trace events are matched to policies in the order specified.
Each policy list should conclude with a default policy -- one that only specifies a sample rate.
This default policy is used to catch remaining trace events that don't match a stricter policy.
Requiring this default policy ensures that traces are only dropped intentionally.
If you enable tail-based sampling and send a transaction that does not match any of the policies,
APM Server will reject the transaction with the error `no matching policy`.

===== Example configuration

This example defines three tail-based sampling polices:

[source, yml]
----
- sample_rate: 1 <1>
  service.environment: production
  trace.name: "GET /very_important_route"
- sample_rate: .01 <2>
  service.environment: production
  trace.name: "GET /not_important_route"
- sample_rate: .1 <3>
----
<1> Samples 100% of traces in `production` with the trace name `"GET /very_important_route"`
<2> Samples 1% of traces in `production` with the trace name `"GET /not_important_route"`
<3> Default policy to sample all remaining traces at 10%, e.g. traces in a different environment, like `dev`,
or traces with any other name

===== Configuration reference

:input-type: tbs
Top-level tail-based sampling configurations:

// This looks like the root service name/env, trace name/env, and trace outcome

[cols="2*<a"]
|===
include::./apm-input-settings.asciidoc[tag=tail_sampling_enabled-setting]
include::./apm-input-settings.asciidoc[tag=tail_sampling_interval-setting]
include::./apm-input-settings.asciidoc[tag=tail_sampling_policies-setting]
|===

Policy configurations:

[cols="2*<a"]
|===
include::./apm-input-settings.asciidoc[tag=sample_rate-setting]
include::./apm-input-settings.asciidoc[tag=trace_name-setting]
include::./apm-input-settings.asciidoc[tag=trace_outcome-setting]
include::./apm-input-settings.asciidoc[tag=service_name-setting]
include::./apm-input-settings.asciidoc[tag=service_env-setting]
|===

:input-type!:
