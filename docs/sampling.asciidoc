[[sampling]]
=== Transaction sampling

Distributed tracing can generate a substantial amount of data.
More data can mean higher costs and more noise.
Sampling aims to lower the amount of data ingested and the effort required to analyze that data --
all while still making it easy to find anomalous patterns in your applications, detect outages, track errors,
and lower mean time to recovery (MTTR).

Elastic APM supports two types of sampling:

* <<head-based-sampling>>
* <<tail-based-sampling>>

[float]
[[head-based-sampling]]
==== Head-based sampling

In head-based sampling, the sampling decision for each trace is made when the trace is initiated.
Each trace has a defined and equal probability of being sampled.

For example, a sampling value of `.2` indicates a transaction sample rate of `20%`.
This means that only `20%` of traces will send and retain all of their associated information.
The remaining traces will drop contextual information to reduce the transfer and storage size of the trace.

Head-based sampling is quick and easy to set up.
Its downside is that it's entirely random -- interesting
data might be discarded purely due to chance.

See <<configure-head-based-sampling>> to get started.

**Distributed tracing with head-based sampling**

In a distributed trace, the sampling decision is still made when the trace is initiated.
Each subsequent service respects the initial service's sampling decision, regardless of its configured sample rate;
the result is a sampling percentage that matches the initiating service.

In this example, `Service A` initiates four transactions and has sample rate of `.5` (`50%`).
The sample rates of `Service B` and `Service C` are ignored.

image::./images/dt-sampling-example-1.png[Distributed tracing and head based sampling example one]

In this example, `Service A` initiates four transactions and has a sample rate of `1` (`100%`).
Again, the sample rates of `Service B` and `Service C` are ignored.

image::./images/dt-sampling-example-2.png[Distributed tracing and head based sampling example two]

**OpenTelemetry with head-based sampling**

Head-based sampling is implemented in the APM agents and SDKs, and
requires the sample rate to be propagated between services and the APM Server.
This functionality is not currently supported by OpenTelemetry,
which results in inaccurate APM throughput, latency, and error metrics.
OpenTelemetry users should consider using tail-based sampling instead.

[float]
[[tail-based-sampling]]
==== Tail-based sampling

In tail-based sampling, the sampling decision for each trace is made after the trace has completed.
This means all traces will be analyzed against a set of rules, or policies, which will determine the rate at which they are sampled.

Unlike head-based sampling, each trace does not have an equal probability of being sampled.
Because slower traces are more interesting than faster ones, tail-based sampling uses weighted random sampling -- so
traces with a longer root transaction duration are more likely to be sampled than traces with a fast root transaction duration.

A downside of tail-based sampling is that it results in more data being sent from APM agents to the APM Server.
The APM Server will therefore use more CPU, memory, and disk than with head-based sampling.
However, because the tail-based sampling decision happens in APM Server, there is less data to transfer from APM Server to {es}.
So running APM Server close to your instrumented services can reduce any increase in transfer costs that tail-based sampling brings.

See <<configure-tail-based-sampling>> to get started.

**Distributed tracing with tail-based sampling**

With tail-based sampling, all traces are observed and a sampling decision is only made once a trace completes.

In this example, `Service A` initiates four transactions.
If our sample rate is `.5` (`50%`) for traces with a `success` outcome,
and `1` (`100%`) for traces with a `failure` outcome,
the sampled traces would look something like this:

image::./images/dt-sampling-example-3.png[Distributed tracing and tail based sampling example one]

**OpenTelemetry with tail-based sampling**

Tail-based sampling is implemented entirely in APM Server,
and will work with traces sent by either Elastic APM agents or OpenTelemetry SDKs.

[float]
=== Sampled data and visualizations

A sampled trace retains all data associated with it.
A non-sampled trace drops all <<data-model-spans,span>> and <<data-model-transactions,transaction>> data^1^.
Regardless of the sampling decision, all traces retain <<data-model-errors,error>> data.

Some visualizations in the {apm-app}, like latency, are powered by aggregated transaction and span <<data-model-metrics,metrics>>.
Metrics are based on sampled traces and weighted by the inverse sampling rate.
For example, if you sample at 5%, each trace is counted as 20.
As a result, as the variance of latency increases, or the sampling rate decreases, your level of error will increase.

^1^ Real User Monitoring (RUM) traces are an exception to this rule.
The {kib} apps that utilize RUM data depend on transaction events,
so non-sampled RUM traces retain transaction data -- only span data is dropped.

[float]
=== Sample rates

What's the best sampling rate? Unfortunately, there isn't one.
Sampling is dependent on your data, the throughput of your application, data retention policies, and other factors.
A sampling rate from `.1%` to `100%` would all be considered normal.
You'll likely decide on a unique sample rate for different scenarios.
Here are some examples:

* Services with considerably more traffic than others might be safe to sample at lower rates
* Routes that are more important than others might be sampled at higher rates
* A production service environment might warrant a higher sampling rate than a development environment
* Failed trace outcomes might be more interesting than successful traces -- thus requiring a higher sample rate

Regardless of the above, cost conscious customers are likely to be fine with a lower sample rate.

[[configure-head-based-sampling]]
==== Configure head-based sampling

There are three ways to adjust the head-based sampling rate of your APM agents:

===== Dynamic configuration

The transaction sample rate can be changed dynamically (no redeployment necessary) on a per-service and per-environment
basis with {kibana-ref}/agent-configuration.html[{apm-agent} Configuration] in {kib}.

===== {kib} API configuration

{apm-agent} configuration exposes an API that can be used to programmatically change
your agents' sampling rate.
An example is provided in the {kibana-ref}/agent-config-api.html[Agent configuration API reference].

===== {apm-agent} configuration

Each agent provides a configuration value used to set the transaction sample rate.
See the relevant agent's documentation for more details:

* Go: {apm-go-ref-v}/configuration.html#config-transaction-sample-rate[`ELASTIC_APM_TRANSACTION_SAMPLE_RATE`]
* Java: {apm-java-ref-v}/config-core.html#config-transaction-sample-rate[`transaction_sample_rate`]
* .NET: {apm-dotnet-ref-v}/config-core.html#config-transaction-sample-rate[`TransactionSampleRate`]
* Node.js: {apm-node-ref-v}/configuration.html#transaction-sample-rate[`transactionSampleRate`]
* PHP: {apm-php-ref-v}/configuration-reference.html#config-transaction-sample-rate[`transaction_sample_rate`]
* Python: {apm-py-ref-v}/configuration.html#config-transaction-sample-rate[`transaction_sample_rate`]
* Ruby: {apm-ruby-ref-v}/configuration.html#config-transaction-sample-rate[`transaction_sample_rate`]

[[configure-tail-based-sampling]]
==== Configure tail-based sampling

Enable tail-based sampling in the <<input-apm,APM integration settings>>.
When enabled, trace events are mapped to sampling policies.
Each sampling policy must specify a sample rate, and can optionally specify other conditions.
All of the policy conditions must be true for a trace event to match it.

Trace events are matched to policies in the order specified.
Each policy list must conclude with a default policy -- one that only specifies a sample rate.
This default policy is used to catch remaining trace events that don't match a stricter policy.
Requiring this default policy ensures that traces are only dropped intentionally.
If you enable tail-based sampling and send a transaction that does not match any of the policies,
APM Server will reject the transaction with the error `no matching policy`.

IMPORTANT: Please note that from version `8.3.1` APM Server implements a default storage limit of 3GB,
but, due to how the limit is calculated and enforced the actual disk space may still grow slightly
over the limit.

===== Example configuration

This example defines three tail-based sampling polices:

[source, yml]
----
- sample_rate: 1 <1>
  service.environment: production
  trace.name: "GET /very_important_route"
- sample_rate: .01 <2>
  service.environment: production
  trace.name: "GET /not_important_route"
- sample_rate: .1 <3>
----
<1> Samples 100% of traces in `production` with the trace name `"GET /very_important_route"`
<2> Samples 1% of traces in `production` with the trace name `"GET /not_important_route"`
<3> Default policy to sample all remaining traces at 10%, e.g. traces in a different environment, like `dev`,
or traces with any other name

===== Configuration reference

:input-type: tbs
**Top-level tail-based sampling settings:**

// This looks like the root service name/env, trace name/env, and trace outcome

[cols="2*<a"]
|===
include::./apm-input-settings.asciidoc[tag=tail_sampling_enabled-setting]
include::./apm-input-settings.asciidoc[tag=tail_sampling_interval-setting]
include::./apm-input-settings.asciidoc[tag=tail_sampling_policies-setting]
|===

**Policy settings:**

[cols="2*<a"]
|===
include::./apm-input-settings.asciidoc[tag=sample_rate-setting]
include::./apm-input-settings.asciidoc[tag=trace_name-setting]
include::./apm-input-settings.asciidoc[tag=trace_outcome-setting]
include::./apm-input-settings.asciidoc[tag=service_name-setting]
include::./apm-input-settings.asciidoc[tag=service_env-setting]
|===

:input-type!:
