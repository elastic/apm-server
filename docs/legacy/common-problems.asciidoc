[[common-problems-legacy]]
== Common problems

IMPORTANT: {deprecation-notice-data}

This section describes common problems you might encounter with APM Server.

* <<no-data-indexed-legacy>>
* <<data-indexed-no-apm-legacy>>
* <<bad-request-legacy>>
* <<event-too-large-legacy>>
* <<unauthorized-legacy>>
* <<forbidden-legacy>>
* <<request-timed-out-legacy>>
* <<ssl-client-fails-legacy>>
* <<field-limit-exceeded-legacy>>
* <<io-timeout-legacy>>
* <<server-es-down-legacy>>
* <<central-config-troubleshooting-legacy>>

[[no-data-indexed-legacy]]
[float]
=== No data is indexed

If no data shows up in {es}, first check that the APM components are properly connected.

To ensure that APM Server configuration is valid and it can connect to the configured output, {es} by default,
run the following commands:

["source","sh"]
------------------------------------------------------------
apm-server test config
apm-server test output
------------------------------------------------------------

To see if the agent can connect to the APM Server, send requests to the instrumented service and look for lines
containing `[request]` in the APM Server logs.

If no requests are logged, it might be that SSL is <<ssl-client-fails-legacy, misconfigured>> or that the host is wrong.
Particularly, if you are using Docker, ensure to bind to the right interface (for example, set
`apm-server.host = 0.0.0.0:8200` to match any IP) and set the `SERVER_URL` setting in the agent accordingly.

If you see requests coming through the APM Server but they are not accepted (response code other than `202`), consider
the response code to narrow down the possible causes (see sections below).

Another reason for data not showing up is that the agent is not auto-instrumenting something you were expecting, check
the {apm-agents-ref}/index.html[agent documentation] for details on what is automatically instrumented.

APM Server currently relies on {es} to create indices that do not exist.
As a result, {es} must be configured to allow {ref}/docs-index_.html#index-creation[automatic index creation] for APM indices.

[[data-indexed-no-apm-legacy]]
[float]
=== Data is indexed but doesn't appear in the APM UI

The {apm-app} relies on index mappings to query and display data.
If your APM data isn't showing up in the {apm-app}, but is elsewhere in {kib}, like the Discover app,
you may have a missing index mapping.

You can determine if a field was mapped correctly with the `_mapping` API.
For example, run the following command in the {kib} {kibana-ref}/console-kibana.html[console].
This will display the field data type of the `service.name` field.

[source,curl]
----
GET apm-*/_mapping/field/service.name
----

If the `mapping.name.type` is `"text"`, your APM indices were not set up correctly.

[source,yml]
----
"mappings" : {
   "service.name" : {
      "full_name" : "service.name",
      "mapping" : {
         "name" : {
            "type" : "text", <1>
            "fields" : {
               "keyword" : {
                  "type" : "keyword",
                  "ignore_above" : 256
               }
            }
         }
      }
   }
}
----
<1> The `service.name` `mapping.name.type` would be `"keyword"` if this field had been set up correctly.

To fix this problem, you must delete and recreate your APM indices as index templates cannot be applied retroactively.

. Stop your APM Server(s) so they are not writing any new documents.

. Delete your existing `apm-*` indices.
In the {kib} console, run:
+
[source,curl]
----
DELETE apm-*
----
+
Alternatively, you can use the {ref}/index-mgmt.html[Index Management] page in {kib}.
Select all `apm-*` index templates and navigate to **Manage Indices** > **Delete Indices**.

. Starting in version 8.0.0, {fleet} uses the APM integration to set up and manage APM index templates.
Install the APM integration by following these steps:
+
--
include::./getting-started-apm-server.asciidoc[tag=install-apm-integration]
--

. Start APM Server.

. Verify the correct index templates were installed. In the {kib} console, run:
+
[source,curl]
----
GET _template/apm-*
----
+
Alternatively, you can use the {ref}/index-mgmt.html[**Index Management**] page in {kib}.
On the **Index Templates** tab, search for `apm` under **Legacy Index Templates**.


[[bad-request-legacy]]
[float]
=== HTTP 400: Data decoding error / Data validation error

The most likely cause for this error is using incompatible versions of {apm-agent} and APM Server.
See the {apm-overview-ref-v}/agent-server-compatibility.html[agent/server compatibility matrix] for more information.

[[event-too-large-legacy]]
[float]
=== HTTP 400: Event too large

APM agents communicate with the APM server by sending events in an HTTP request. Each event is sent as its own line in the HTTP request body. If events are too large, you should consider increasing the <<max_event_size,`max_event_size`>>
setting in the APM Server, and adjusting relevant settings in the agent.

[[unauthorized-legacy]]
[float]
=== HTTP 401: Invalid token

The <<secret-token-legacy, secret token>> in the request header doesn't match the configured in the APM Server.

[[forbidden-legacy]]
[float]
=== HTTP 403: Forbidden request

Either you are sending requests to a <<configuration-rum, RUM>> endpoint without RUM enabled, or a request
is coming from an origin not specified in `apm-server.rum.allow_origins`. See the <<configuration-rum, RUM configuration>>.

[[request-timed-out-legacy]]
[float]
=== HTTP 503: Request timed out waiting to be processed

This happens when APM Server exceeds the maximum number of requests that it can process concurrently.

To alleviate this problem, you can try to:

* <<reduce-sample-rate>>
* <<reduce-stacktrace>>
* <<reduce-payload-size>>
* <<add-apm-server-instances>>

[float]
[[ssl-client-fails-legacy]]
=== SSL client fails to connect

The target host running might be unreachable or the certificate may not be valid. To resolve your issue:

* Make sure that the APM Server process on the target host is running and you can connect to it.
First, try to ping the target host to verify that you can reach it from the host running {beatname_uc}.
Then use either `nc` or `telnet` to make sure that the port is available. For example:
+
[source,shell]
----------------------------------------------------------------------
ping <hostname or IP>
telnet <hostname or IP> 5044
----------------------------------------------------------------------

* Verify that the certificate is valid and that the hostname and IP match.

* Use OpenSSL to test connectivity to the target server and diagnose problems.
See the https://www.openssl.org/docs/manmaster/apps/s_client.html[OpenSSL documentation] for more info.

[float]
==== Common SSL-Related Errors and Resolutions

Here are some common errors and ways to fix them:

* <<cannot-validate-certificate-legacy,x509: cannot validate certificate>>
* <<getsockopt-no-route-to-host-legacy,getsockopt: no route to host>>
* <<getsockopt-connection-refused-legacy,getsockopt: connection refused>>
* <<target-machine-refused-connection-legacy,No connection could be made because the target machine actively refused it>>

[float]
[[cannot-validate-certificate-legacy]]
===== x509: cannot validate certificate for <IP address> because it doesn't contain any IP SANs

This happens because your certificate is only valid for the hostname present in the Subject field.

To resolve this problem, try one of these solutions:

* Create a DNS entry for the hostname, mapping it to the server's IP.
* Create an entry in `/etc/hosts` for the hostname. Or, on Windows, add an entry to
`C:\Windows\System32\drivers\etc\hosts`.
* Re-create the server certificate and add a Subject Alternative Name (SAN) for the IP address of the server. This makes the
server's certificate valid for both the hostname and the IP address.

[float]
[[getsockopt-no-route-to-host-legacy]]
===== getsockopt: no route to host

This is not an SSL problem. It's a networking problem. Make sure the two hosts can communicate.

[float]
[[getsockopt-connection-refused-legacy]]
===== getsockopt: connection refused

This is not an SSL problem. Make sure that {ls} is running and that there is no firewall blocking the traffic.

[float]
[[target-machine-refused-connection-legacy]]
===== No connection could be made because the target machine actively refused it

A firewall is refusing the connection. Check if a firewall is blocking the traffic on the client, the network, or the
destination host.

[[field-limit-exceeded-legacy]]
[float]
=== Field limit exceeded

When adding too many distinct tag keys on a transaction or span,
you risk creating a link:{ref}/mapping.html#mapping-limit-settings[mapping explosion].

For example,
you should avoid that user-specified data,
like URL parameters,
is used as a tag key.
Likewise,
using the current timestamp or a user ID as a tag key is not a good idea.
However,
tag *values* with a high cardinality are not a problem.
Just try to keep the number of distinct tag keys at a minimum.

The symptom of a mapping explosion is that transactions and spans are not indexed anymore after a certain time.
Usually,
on the next day,
the spans and transactions will be indexed again because a new index is created each day.
But as soon as the field limit is reached,
indexing stops again.

In the agent logs,
you won't see a sign of failures as the APM server asynchronously sends the data it received from the agents to {es}.
However,
the APM server and {es} log a warning like this:

[source,logs]
----
{\"type\":\"illegal_argument_exception\",\"reason\":\"Limit of total fields [1000] in index [apm-7.0.0-transaction-2017.05.30] has been exceeded\"}
----

[[io-timeout-legacy]]
[float]
=== I/O Timeout

I/O Timeouts can occur when your timeout settings across the stack are not configured correctly,
especially when using a load balancer.

You may see an error like the one below in the agent logs, and/or a similar error on the APM Server side:

[source,logs]
----------------------------------------------------------------------
[ElasticAPM] APM Server responded with an error:
"read tcp 123.34.22.313:8200->123.34.22.40:41602: i/o timeout"
----------------------------------------------------------------------

To fix this, ensure timeouts are incrementing from the {apm-agents-ref}[{apm-agent}],
through your load balancer, to the <<read_timeout,APM Server>>.

By default, the agent timeouts are set at 10 seconds, and the server timeout is set at 30 seconds.
Your load balancer should be set somewhere between these numbers.

For example:

[source,txt]
----------------------------------------------------------------------
APM agent --> Load Balancer  --> APM Server
   10s            15s               30s
----------------------------------------------------------------------

[[server-es-down-legacy]]
[float]
=== What happens when APM Server or {es} is down?

*If {es} is down*

APM Server does not have an internal queue to buffer requests,
but instead leverages an HTTP request timeout to act as back-pressure.
If {es} goes down, the APM Server will eventually deny incoming requests.
Both the APM Server and {apm-agent}(s) will issue logs accordingly.

*If APM Server is down*

Some agents have internal queues or buffers that will temporarily store data if the APM Server goes down.
As a general rule of thumb, queues fill up quickly. Assume data will be lost if APM Server goes down.
Adjusting these queues/buffers can increase the agent's overhead, so use caution when updating default values.

* **Go agent** - Circular buffer with configurable size:
{apm-go-ref}/configuration.html#config-api-buffer-size[`ELASTIC_APM_BUFFER_SIZE`].
// * **iOS agent** - ??
* **Java agent** - Internal buffer with configurable size:
{apm-java-ref}/config-reporter.html#config-max-queue-size[`max_queue_size`].
* **Node.js agent** - No internal queue. Data is lost.
* **PHP agent** - No internal queue. Data is lost.
* **Python agent** - Internal {apm-py-ref}/tuning-and-overhead.html#tuning-queue[Transaction queue]
with configurable size and time between flushes.
* **Ruby agent** - Internal queue with configurable size:
{apm-ruby-ref}/configuration.html#config-api-buffer-size[`api_buffer_size`].
* **RUM agent** - No internal queue. Data is lost.
* **.NET agent** - No internal queue. Data is lost.

[[central-config-troubleshooting-legacy]]
[float]
=== `/api/apm/settings/agent-configuration/search` errors

If you're instrumenting and starting a lot of services at the same time
or using a very large number of service or environment names,
you may see the following APM Server logs related to {apm-agent} central configuration:

* `.../api/apm/settings/agent-configuration/search: context canceled`
* `.../api/apm/settings/agent-configuration/search: net/http: TLS handshake timeout`

There are two possible causes:

1. {kib} is overwhelmed by the number of requests coming from APM Server.
2. {es} can't reply quickly enough to {kib}.

For cause #1, try one or more of the following:

* Increase the <<elasticsearch-output,`apm-server.kibana.timeout`>> setting.
* Increase the <<setup-kibana-endpoint,`agent.config.cache.expiration`>>.
* Increase {kib}'s resources so that it is able to manage more requests.
* If you're not using APM central configuration, disable it with <<setup-kibana-endpoint,`apm-server.kibana.enabled`>>.
Central configuration can also be disabled at the {apm-agent} level.

For cause #2, investigate why {es} is not responding in a timely manner.
{kib}'s queries to {es} are simple, so it may just be that {es} is unhealthy.
If that's not the problem, you may need to use {ref}/index-modules-slowlog.html[Search Slow Log] to investigate your {es} logs.

To avoid this problem entirely,
we recommend <<upgrade-to-apm-integration,upgrading to the Elastic APM integration>>.
