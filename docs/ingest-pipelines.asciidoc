//////////////////////////////////////////////////////////////////////////
// This content is reused in the Legacy ingest pipeline
//////////////////////////////////////////////////////////////////////////

[[ingest-pipelines]]
=== Parse data using ingest pipelines

:append-legacy:
// tag::ingest-pipelines[]

Ingest pipelines preprocess and enrich APM documents before indexing them.
For example, a pipeline might define one processor that removes a field,
one that transforms a field, and another that renames a field.

The default APM pipelines are defined in index templates that {fleet} loads into {es}.
{es} then uses the index pattern in these index templates to match pipelines to APM data streams.

[discrete]
[id="custom-ingest-pipelines{append-legacy}"]
=== Custom ingest pipelines

The Elastic APM integration supports custom ingest pipelines.
A custom pipeline allows you to transform data to better match your specific use case.
This can be useful, for example, to ensure data security by removing or obfuscating sensitive information.

Each data stream ships with a default pipeline.
This default pipeline calls an initially non-existent and non-versioned "`@custom`" ingest pipeline.
If left uncreated, this pipeline has no effect on your data. However, if utilized,
this pipeline can be used for custom data processing, adding fields, sanitizing data, and more.

In addition, ingest pipelines can also be used to direct application metrics (`metrics-apm.app.*`) to a data stream with a different dataset, e.g. to combine metrics for two applications.
Sending other APM data to alternate data streams, like traces (`traces-apm.*`), logs (`logs-apm.*`), and internal metrics (`metrics-apm.internal*`) is not currently supported.

[discrete]
[id="custom-ingest-pipeline-naming{append-legacy}"]
=== `@custom` ingest pipeline naming convention

// tag::ingest-pipeline-naming[]
`@custom` pipelines are specific to each data stream and follow a similar naming convention: `<type>-<dataset>@custom`.
As a reminder, the default APM data streams are:

include::./data-streams.asciidoc[tag=traces-data-streams]
include::./data-streams.asciidoc[tag=metrics-data-streams]
include::./data-streams.asciidoc[tag=logs-data-streams]

To match a custom ingest pipeline with a data stream, follow the `<type>-<dataset>@custom` template,
or replace `-namespace` with `@custom` in the table above.
For example, to target application traces, you'd create a pipeline named `traces-apm@custom`.
// end::ingest-pipeline-naming[]

The `@custom` pipeline can directly contain processors or you can use the
pipeline processor to call other pipelines that can be shared across multiple data streams or integrations.
The `@custom` pipeline will persist across all version upgrades.

[discrete]
[id="custom-ingest-pipeline-create{append-legacy}"]
=== Create a `@custom` ingest pipeline

The process for creating a custom ingest pipeline is as follows:

* Create a pipeline with processors specific to your use case
* Add the newly created pipeline to an `@custom` pipeline that matches an APM data stream
* Roll over your data stream

If you prefer more guidance, see one of these tutorials:

* <<filters-ingest-pipeline>> — An APM-specific tutorial where you learn how to obfuscate passwords stored in the `http.request.body.original` field.
* {fleet-guide}/data-streams-pipeline-tutorial.html[Transform data with custom ingest pipelines] — A basic Elastic integration tutorial where you learn how to add a custom field to incoming data.

// end::ingest-pipelines[]