[[logstash-output]]
== Configure the {ls} output

++++
<titleabbrev>{ls}</titleabbrev>
++++

****
image:./binary-yes-fm-no.svg[supported deployment methods]

The {ls} output is not yet supported by {fleet}-managed APM Server.
****

{ls} allows for additional processing and routing of APM events.
The {ls} output sends events directly to {ls} using the lumberjack
protocol, which runs over TCP.

[float]
== Send events to {ls}

To send events to {ls}, you must:

. <<ls-output-config,Enable the {ls} output in APM Server>>
. <<ls-kib-config,Enable the {kib} endpoint in APM Server>>
. <<ls-config-pipeline,Create a {ls} configuration pipeline in {ls}>>

[float]
[[ls-output-config]]
=== {ls} output configuration

To enable the {ls} output in APM Server,
edit the `apm-server.yml` file to:

. Disable the {es} output by commenting it out and
. Enable the {ls} output by uncommenting the {ls} section and setting `enabled` to `true`:
+
[source,yaml]
----
output.logstash:
  enabled: true
  hosts: ["localhost:5044"] <1>
----
<1> The `hosts` option specifies the {ls} server and the port (`5044`) where {ls} is configured to listen for incoming
APM Server connections.

[float]
[[ls-kib-config]]
=== {kib} endpoint configuration

include::../../shared-kibana-endpoint.asciidoc[tag=shared-kibana-config]

[float]
[[ls-config-pipeline]]
=== {ls} configuration pipeline

Finally, you must create a {ls} configuration pipeline that listens for incoming
APM Server connections, dedots the `data_stream.*` fields, and indexes received events into {es}.

. Use the {logstash-ref}/plugins-inputs-elastic_agent.html[Elastic Agent input plugin] to configure
{ls} to receive events from the APM Server. A minimal `input` config might look like this:
+
[source,conf]
----
input {
  elastic_agent {
    port => 5044
  }
}
----

. Use the {logstash-ref}/plugins-filters-mutate.html[Mutate filter plugin] to set up <<apm-data-streams,data streams>>.
Because the {ls} {es} output doesn't understand dotted field notation, you must use this filter to
dedot the default `data_stream.*` fields sent from APM Server to {ls}.
+
[source,conf]
----
filter {
  mutate {
    rename => {
      "[data_stream.type]" => "[data_stream][type]"
      "[data_stream.dataset]" => "[data_stream][dataset]"
      "[data_stream.namespace]" => "[data_stream][namespace]"
    }
  }
}
----
+
.Expand to learn more
[%collapsible]
====
****
APM Server sends data stream information to {ls} in the following format:

[source,json]
----
{
  "data_stream.dataset": "apm",
  "data_stream.type": "traces",
  "data_stream.namespace": "default"
}
----

{es} expects to receive data stream information in the following format:

[source,json]
----
"data_stream" {
  "dataset": "apm",
  "type": "traces",
  "dataset": "default"
}
----

The mutation defined above transforms what APM Server sends to {ls} into a data format that {es} understands.
This allows you to automatically route APM data to the appropriate data streams.
****
====

. Use the {logstash-ref}/plugins-outputs-elasticsearch.html[{es} output plugin] to send
events to {es} for indexing. A minimal `output` config might look like this:
+
[source,conf]
----
output {
  elasticsearch {
    data_stream => "true" <1>
    cloud_id => "YOUR_CLOUD_ID_HERE" <2>
    cloud_auth => "YOUR_CLOUD_AUTH_HERE" <2>
  }
}
----
<1> Enables indexing into {es} data streams.
<2> This example assumes you're sending data to {ecloud}. If you're using a self-hosted version of {es}, use `hosts` instead. See {logstash-ref}/plugins-outputs-elasticsearch.html[{es} output plugin] for more information.

Here's what your basic {ls} configuration file will look like when we put everything together:

[source,conf]
----
input {
  elastic_agent {
    port => 5044
  }
}

filter {
  mutate {
    rename => {
      "[data_stream.type]" => "[data_stream][type]"
      "[data_stream.dataset]" => "[data_stream][dataset]"
      "[data_stream.namespace]" => "[data_stream][namespace]"
    }
  }
}

output {
  elasticsearch {
    data_stream => "true"
    cloud_id => "YOUR_CLOUD_ID_HERE"
    cloud_auth => "YOUR_CLOUD_AUTH_HERE"
  }
}
----

[float]
== Accessing the @metadata field

Every event sent to {ls} contains a special field called
{logstash-ref}/event-dependent-configuration.html#metadata[`@metadata`] that you can
use in {ls} for conditionals, filtering, indexing and more.
APM Server sends the following `@metadata` to {ls}:

["source","json",subs="attributes"]
----
{
    ...
    "@metadata": {
      "beat": "apm-server", <1>
      "version": "{version}" <2>
    }
}
----
<1> To change the default `apm-server` value, set the
<<logstash-index,`index`>> option in the APM Server config file.
<2> The current version of APM Server.

In addition to `@metadata`, APM Server provides other potentially useful fields, like the
`data_stream` field, which can be used to conditionally operate on
{apm-guide-ref}/data-model.html[event types], namespaces, or datasets.

As an example, you might want to use {ls} to route all `metric` events to the same custom metrics data stream,
rather than to service-specific data streams:

["source","json",subs="attributes"]
----
output {
  if [@metadata][beat] == "apm-server" { <1>
    if [data_stream][type] == "metrics" { <2>
      elasticsearch {
        index => "%{[data_stream][type]}-custom-%{[data_stream][namespace]}" <3>
        action => "create" <4>
        cloud_id => "${CLOUD_ID}" <5>
        cloud_auth => "${CLOUD_AUTH}" <5>
      }
    } else {
      elasticsearch {
        data_stream => "true" <6>
        cloud_id => "${CLOUD_ID}"
        cloud_auth => "${CLOUD_AUTH}"
      }
    }
  }
}
----
<1> Only apply this output if the data is being sent from the APM Server
<2> Determine if the event type is `metric`
<3> If the event type is `metric`, output to a custom data stream: `metrics-custom-<YOUR_NAMESPACE>`
<4> You must explicitly set `action` to `create when using {ls} to output an index to a data stream
<5> In this example, `cloud_id` and `cloud_auth` are stored as {logstash-ref}/environment-variables.html[environment variables]
<6> For all other event types, index data directly into the predefined APM data steams

=== Compatibility

This output works with all compatible versions of {ls}. See the
https://www.elastic.co/support/matrix#matrix_compatibility[Elastic Support
Matrix].

=== Configuration options

You can specify the following options in the `logstash` section of the
+{beatname_lc}.yml+ config file:

==== `enabled`

The enabled config is a boolean setting to enable or disable the output. If set
to false, the output is disabled.

The default value is `false`.

[[hosts]]
==== `hosts`

The list of known {ls} servers to connect to. If load balancing is disabled, but
multiple hosts are configured, one host is selected randomly (there is no precedence).
If one host becomes unreachable, another one is selected randomly.

All entries in this list can contain a port number. The default port number 5044 will be used if no number is given.

==== `compression_level`

The gzip compression level. Setting this value to 0 disables compression.
The compression level must be in the range of 1 (best speed) to 9 (best compression).

Increasing the compression level will reduce the network usage but will increase the CPU usage.

The default value is 3.

==== `escape_html`

Configure escaping of HTML in strings. Set to `true` to enable escaping.

The default value is `false`.

==== `worker`

The number of workers per configured host publishing events to {ls}. This
is best used with load balancing mode enabled. Example: If you have 2 hosts and
3 workers, in total 6 workers are started (3 for each host).

[[loadbalance]]
==== `loadbalance`

If set to true and multiple {ls} hosts are configured, the output plugin
load balances published events onto all {ls} hosts. If set to false,
the output plugin sends all events to only one host (determined at random) and
will switch to another host if the selected one becomes unresponsive. The default value is false.

["source","yaml",subs="attributes"]
------------------------------------------------------------------------------
output.logstash:
  hosts: ["localhost:5044", "localhost:5045"]
  loadbalance: true
  index: {beatname_lc}
------------------------------------------------------------------------------

==== `ttl`

Time to live for a connection to {ls} after which the connection will be re-established.
Useful when {ls} hosts represent load balancers. Since the connections to {ls} hosts
are sticky, operating behind load balancers can lead to uneven load distribution between the instances.
Specifying a TTL on the connection allows to achieve equal connection distribution between the
instances.  Specifying a TTL of 0 will disable this feature.

The default value is 0.

NOTE: The "ttl" option is not yet supported on an asynchronous {ls} client (one with the "pipelining" option set).

==== `pipelining`

Configures the number of batches to be sent asynchronously to {ls} while waiting
for ACK from {ls}. Output only becomes blocking once number of `pipelining`
batches have been written. Pipelining is disabled if a value of 0 is
configured. The default value is 2.

==== `proxy_url`

The URL of the SOCKS5 proxy to use when connecting to the {ls} servers. The
value must be a URL with a scheme of `socks5://`. The protocol used to
communicate to {ls} is not based on HTTP so a web-proxy cannot be used.

If the SOCKS5 proxy server requires client authentication, then a username and
password can be embedded in the URL as shown in the example.

When using a proxy, hostnames are resolved on the proxy server instead of on the
client. You can change this behavior by setting the
<<logstash-proxy-use-local-resolver,`proxy_use_local_resolver`>> option.

["source","yaml",subs="attributes"]
------------------------------------------------------------------------------
output.logstash:
  hosts: ["remote-host:5044"]
  proxy_url: socks5://user:password@socks5-proxy:2233
------------------------------------------------------------------------------

[[logstash-proxy-use-local-resolver]]
==== `proxy_use_local_resolver`

The `proxy_use_local_resolver` option determines if {ls} hostnames are
resolved locally when using a proxy. The default value is false, which means
that when a proxy is used the name resolution occurs on the proxy server.

[[logstash-index]]
==== `index`

The index root name to write events to. The default is `apm-server`. For
example +"{beat_default_index_prefix}"+ generates +"[{beat_default_index_prefix}-]{version}-YYYY.MM.DD"+
indices (for example, +"{beat_default_index_prefix}-{version}-2017.04.26"+).

NOTE: This parameter's value will be assigned to the `metadata.beat` field. It
can then be accessed in {ls}'s output section as `%{[@metadata][beat]}`.

==== `ssl`

Configuration options for SSL parameters like the root CA for {ls} connections. See
<<configuration-ssl>> for more information. To use SSL, you must also configure the
{logstash-ref}/plugins-inputs-beats.html[{beats} input plugin for {ls}] to use SSL/TLS.

==== `timeout`

The number of seconds to wait for responses from the {ls} server before timing out. The default is 30 (seconds).

==== `max_retries`

The number of times to retry publishing an event after a publishing failure.
After the specified number of retries, the events are typically dropped.

Set `max_retries` to a value less than 0 to retry until all events are published.

The default is 3.

==== `bulk_max_size`

The maximum number of events to bulk in a single {ls} request. The default is 2048.

If the Beat sends single events, the events are collected into batches. If the Beat publishes
a large batch of events (larger than the value specified by `bulk_max_size`), the batch is
split.

Specifying a larger batch size can improve performance by lowering the overhead of sending events.
However big batch sizes can also increase processing times, which might result in
API errors, killed connections, timed-out publishing requests, and, ultimately, lower
throughput.

Setting `bulk_max_size` to values less than or equal to 0 disables the
splitting of batches. When splitting is disabled, the queue decides on the
number of events to be contained in a batch.

==== `slow_start`

If enabled, only a subset of events in a batch of events is transferred per transaction.
The number of events to be sent increases up to `bulk_max_size` if no error is encountered.
On error, the number of events per transaction is reduced again.

The default is `false`.

==== `backoff.init`

The number of seconds to wait before trying to reconnect to {ls} after
a network error. After waiting `backoff.init` seconds, {beatname_uc} tries to
reconnect. If the attempt fails, the backoff timer is increased exponentially up
to `backoff.max`. After a successful connection, the backoff timer is reset. The
default is `1s`.

==== `backoff.max`

The maximum number of seconds to wait before attempting to connect to
{ls} after a network error. The default is `60s`.

// Logstash security
include::{docdir}/shared-ssl-logstash-config.asciidoc[]
